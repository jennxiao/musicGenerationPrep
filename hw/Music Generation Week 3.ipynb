{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3:\n",
    "**For Codeology's Music Generation Project Fall 2019 by [Jennifer Xiao](mailto:jenniferxiao@berkeley.edu) and [Alma Pineda](mailto:almapineda@berkeley.edu)**\n",
    "\n",
    "**With help from Codeology's ML workshop Jupyter notebook authored [Calvin Chen](mailto:chencalvin99@berkeley.edu), [Micah Harrison](mailto:mharrison08@berkeley.edu), and [Sai Kapuluru](mailto:saikapuluru@berkeley.edu).**\n",
    "\n",
    "### Table Of Contents\n",
    "* [Review of last week](#review)\n",
    "* [Jupyter Notebook guide](#intro)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='review'></a>\n",
    "## Review from last week\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who weren't here last week, we'll be touching upon some of the same topics from last time, so we'll take a quick minute or so to get you guys up to speed with what's needed so you guys'll know what's going on!\n",
    "\n",
    "**Loss Functions**: A function that calculates how inaccurate your model is against some given dataset. The one particular function we'll be using today is the **Mean Squared Error**, where we essentially **find the average of the squares between the actual values and our predicted values**. Don't worry if some parts of this don't make that much sense now, we'll be getting more into Loss Functions later on, and how they apply to different models to help optimize them!\n",
    "\n",
    "**Training Our Model**: We'll refer to the process of training our model time and time again, but this can essentially be abstracted away as the process through which **our model is fitting itself against our given dataset**, allowing it to better predict other similar inputs in the future better.\n",
    "\n",
    "**Validation**: A process through which we're able to determine how well our model is doing. We split our dataset into thirds: **60% of dataset to the training data**, **20% of the dataset to the validation set**, and **20% of the dataset to the test set**. We train a bunch of different models on the training set, determine which one has the lowest *loss*, or highest accuracy, on the validation set, and finally choose the most accurate one and determine how well it did against the final **test set**.\n",
    "\n",
    "Now that we've defined a few of the key terms from last week that we'll be using this week, let's get into the new stuff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Neural Networks + The Motivaiton Behind Creating One\n",
    "\n",
    "Neural Networks derive their name form the neural network we have in our head- our brain. In a very highly simplified model, the brain is a collection of neurons that receives electrical input signals from dendrites, outputting electrical signals via a single axon. Each neuron sends signals along a single axon and connects with other dendrites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks attempt to model this by having artificial layers of neurons that perform minor tasks, then communicate from layer to layer. The collection of neurons operate together to begin to understand the raw input in layers- in each layer, the neurons summarize the data a little bit further, until they finally arrive at a single label. So this process of gradual learning is the reason why we're motivated to construct such a network (especially also because the neural networks in our brain work so well), and why we construct such networks in Machine Learning!\n",
    "\n",
    "So, this process of gradual learning basically sums up the **feed-forward Neural Network**, which is what we'll be exploring today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to learn about how a Neural Networks, we can break it down into three key components:\n",
    "1. **The Single Neuron + The Single Layer**\n",
    "2. **Forward Propagation**\n",
    "    1. **Activation Functions**\n",
    "    2. **Loss Functions**\n",
    "    3. **Gradient Descent**\n",
    "3. **Backpropagation**\n",
    "\n",
    "We'll be diving deep into all three of the topics today, so get to learn about one of the biggest buzzwords in Machine Learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A single neuron transforms given input into some output. Depending on the given input and weights assigned to each input, decide whether the neuron fired or not. Let’s assume the neuron has 3 input connections and one output\" (Wikipedia.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple Neural Network. There are two input, two hidden, and one output node. Information flows from the input nodes to the hidden nodes and finally to the output node. w stands for weight: they represent how much of an effect an input has on the next node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Proagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ascelibrary.org/cms/attachment/bf8cb032-91be-4396-8f14-ba053335ac3c/figure2.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable. They introduce non-linear properties to our Network. Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.\n",
    "\n",
    "Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply a Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer.\n",
    "\n",
    "If we do not apply a Activation function then the output signal would simply be a simple linear function. Linear functions have their use (as shown last time) but they are limited by their simplicity. Non-linear functions help to capture patterns in data that might be too complex for a linear model to recognize.\n",
    "\n",
    "Also another important feature of a Activation function is that it should be differentiable for backpropagation later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "Sigmoid Activation function: It is a activation function of form f(x) = 1 / 1 + exp(-x) . Its Range is between 0 and 1. It is a S — shaped curve. It is easy to understand and apply but it has major reasons which have made it fall out of popularity -\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "x = np.arange(-4, 4, 0.01)\n",
    "\n",
    "def sigmoid_array(x):                                        \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "sig = sigmoid_array(x)\n",
    "plt.axvline(color='black')\n",
    "plt.axhline(color='black')\n",
    "plt.axhline(np.median(sig), color = 'g', linestyle = '--')\n",
    "plt.ylim(-.5, 1)\n",
    "plt.plot(x, sig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu\n",
    "\n",
    "It’s just R(x) = max(0,x) i.e if x < 0 , R(x) = 0 and if x >= 0 , R(x) = x. Hence as seeing the mathamatical form of this function we can see that it is very simple and efficinent . A lot of times in Machine learning and computer science we notice that most simple and consistent techniques and methods are only preferred and are best. Almost all deep learning Models use ReLu nowadays. But its limitation is that it should only be used within Hidden layers of a Neural Network Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-4, 4, 0.01)\n",
    "\n",
    "def relu(x):                                        \n",
    "    return [max(0,elem) for elem in x]\n",
    "\n",
    "re = relu(x)\n",
    "plt.axvline(color='black', linestyle = \"--\")\n",
    "plt.axhline(color='black', linestyle = \"--\")\n",
    "plt.ylim(-.5, 1)\n",
    "plt.plot(x, re)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "In order to monitor our progress and make sure that we are moving in right direction, we should routinely calculate the value of the loss function. Generally speaking, the loss function is designed to show how far we are from the ‘ideal’ solution. It is selected according to the problem we plan to solve, and frameworks such as Pytorch have many options to choose from.\n",
    "\n",
    "\n",
    "Each of the neural network's weights receives an update proportional to the partial derivative of the loss function with respect to the current weight in each iteration of training\n",
    "\n",
    "Vanashing Gradient Problem: the gradient will be vanishingly small, effectively preventing the weight from changing its value.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\mathcal{L}}(\\boldsymbol{\\theta})=\\frac{1}{n}\\sum_{i=1}^{n}L\\big(y^{(i)},f(\\mathbf{x}^{(i)},\\boldsymbol{\\theta})\\big)\\newline\n",
    "f(\\mathbf{x}^{(i)},\\boldsymbol{\\theta}) = \\hat{y}^{(i)} \\newline\n",
    "L\\big(y^{(i)},f(\\mathbf{x}^{(i)},\\boldsymbol{\\theta}))=\\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)}-\\hat{y}^{(i)})^{2}\\newline\n",
    "\\frac{\\partial\\boldsymbol{\\mathcal{L}}}{\\partial\\boldsymbol{\\theta}}=-(y-\\sigma(\\mathbf{z}))\\cdot\\sigma’(\\mathbf{z})\\cdot\\mathbf{x}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{\\theta}^{*} & =\\arg\\min_{\\boldsymbol{\\theta}}\\boldsymbol{\\mathcal{L}}(\\boldsymbol{\\theta})+\\lambda\\cdot\\Phi(\\boldsymbol{\\theta})\\newline\n",
    "& =\\arg\\min_{\\boldsymbol{\\theta}}\\frac{1}{n}\\sum_{i=1}^{n}L\\big(y^{(i)},\\hat{y}^{(i)}\\big)+\\lambda\\cdot\\Phi(\\boldsymbol{\\theta})\\newline\n",
    "& =\\arg\\min_{\\boldsymbol{\\theta}}\\frac{1}{n}\\sum_{i=1}^{n}L\\big(y^{(i)},f(\\mathbf{x}^{(i)},\\boldsymbol{\\theta})\\big)+\\lambda\\cdot\\Phi(\\boldsymbol{\\theta})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation\n",
    "\n",
    "\n",
    "Now that we’ve measured the error of our prediction (loss), we need to find a way to propagate the error back, and to update our weights and biases.\n",
    "In order to know the appropriate amount to adjust the weights and biases by, we need to know the derivative of the loss function with respect to the weights and biases.\n",
    "\n",
    "So, each iteration of the training process consists of the following steps:\n",
    "1. Calculating the predicted output ŷ, known as feedforward\n",
    "2. Updating the weights and biases, known as backpropagation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO TIME BABY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create some data. We create the data that represents the X^2 curve. We generate 1000 points of this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    randX = random.uniform(-10, 10)\n",
    "    X.append(randX)\n",
    "    Y.append(randX ** 2)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "Y += np.random.normal(0, 0.1, len(Y))\n",
    "\n",
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our Neural Net. Our goal is to model the underlying distribution (X^2) using our neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lass Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(1, 120)\n",
    "        self.fc2 = nn.Linear(120, 120)\n",
    "        self.fc3 = nn.Linear(120, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.0001)\n",
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets train for 5000 iterations\n",
    "for i in range(5000):\n",
    "    dataIndices = [random.randint(0, 999)]\n",
    "    Xdata = torch.tensor(X[dataIndices], dtype=torch.float)\n",
    "    predictions = net(Xdata)\n",
    "    labels = torch.tensor(Y[dataIndices], dtype=torch.float)\n",
    "    loss = loss_func(predictions, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "#Once our model is all trained, we plot 1000 points uniformly \n",
    "# and see what kind of distribution our model has captured. \n",
    "# We want this to be as close to the true underlying distribution (X^2) as possible.\n",
    "\n",
    "\n",
    "x = np.linspace(-10, 10, num=1000)\n",
    "predictions = []\n",
    "for i in range(1000):\n",
    "    y = net(torch.tensor([x[i]], dtype=torch.float)).data[0].item()\n",
    "    predictions.append(y)\n",
    "    \n",
    "plt.scatter(x, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
